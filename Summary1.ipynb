{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.计算权重参数和偏置的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image Name](https://cdn.kesci.com/upload/image/q5ho684jmh.png)\n",
    "\n",
    "给定一个小批量样本$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$，其批量大小为$n$，输入个数为$d$，输出个数为$q$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\\boldsymbol{H}$，有$\\boldsymbol{H} \\in \\mathbb{R}^{n \\times h}$。因为隐藏层和输出层均是全连接层，则隐藏层的权重参数和偏差参数分别为$\\boldsymbol{W}_h \\in \\mathbb{R}^{d \\times h}$和 $\\boldsymbol{b}_h \\in \\mathbb{R}^{1 \\times h}$，输出层的权重和偏差参数分别为$\\boldsymbol{W}_o \\in \\mathbb{R}^{h \\times q}$和$\\boldsymbol{b}_o \\in \\mathbb{R}^{1 \\times q}$。\n",
    "\n",
    "`参数的形状与批量大小无关`\n",
    "\n",
    "例：对于只含有一个隐藏层的多层感知机，输入是256×256的图片，隐藏单元个数是1000，输出类别个数是10，求模型的所有权重矩阵$W_{i}$的元素数量之和。\n",
    "\n",
    "解：图片256×256展平为65536，65536×1000+1000×10=65546000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "broadcast机制：广播仅仅是一组用于在不同大小的数组上应用二元ufuncs(加法、减法、乘法等)的规则。\n",
    "\n",
    "* 规则1:如果两个数组在维度(dim=0标量, dim=1向量, dim=2矩阵)的数量上有差异，那么维度较少的数组的形状就会被用1填充在它的前导(左)边。\n",
    "* 规则2:如果两个数组的形状在任何维度上都不匹配，但某个维度等于1，那么在这个维度中，形状为1的数组将被拉伸以匹配另一个形状。\n",
    "* 规则3:如果在任何维度上，大小都不一致，且两者都不等于1，就会出现错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "(2, 3)\n",
      "[0 1 2]\n",
      "(3,)\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "M = np.ones((2, 3))\n",
    "print(M)\n",
    "print(M.shape)\n",
    "# M.shape -> (2, 3)\n",
    "\n",
    "a = np.arange(3)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "# a.shape -> (3, )\n",
    "\n",
    "#根据规则1，数组a的维数更少，所以用1填充在它的左边，\n",
    "# a.shape -> (1, 3)\n",
    "\n",
    "#根据规则2，可将a变为 a.shape -> (2, 3)\n",
    "x = (M + a).shape\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵（cross entropy）是一个常用的衡量两个概率分布差异的测量函数：\n",
    "\n",
    "\n",
    "$$\n",
    "H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},\n",
    "$$\n",
    "\n",
    "\n",
    "其中带下标的$y_j^{(i)}$是向量$\\boldsymbol y^{(i)}$中非0即1的元素，需要注意将它与样本$i$类别的离散数值，即不带下标的$y^{(i)}$区分。在上式中，我们知道向量$\\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素$y^{(i)}{y^{(i)}}$为1，其余全为0，于是$H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}) = -\\log \\hat y_{y^{(i)}}^{(i)}$。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。\n",
    "\n",
    "假设训练数据集的样本数为$n$，交叉熵损失函数定义为 \n",
    "$$\n",
    "\\ell(\\boldsymbol{\\Theta}) = \\frac{1}{n} \\sum_{i=1}^n H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ),\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\boldsymbol{\\Theta}$代表模型参数。同样地，如果每个样本只有一个标签，那么交叉熵损失可以简写成$\\ell(\\boldsymbol{\\Theta}) = -(1/n) \\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)}$。从另一个角度来看，我们知道最小化$\\ell(\\boldsymbol{\\Theta})$等价于最大化$\\exp(-n\\ell(\\boldsymbol{\\Theta}))=\\prod_{i=1}^n \\hat y_{y^{(i)}}^{(i)}$，即最小化交叉熵损失函数等价于最大化训练数据集所有标签类别的联合预测概率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.多维Tensor按维度操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(X)\n",
    "print(X.sum(dim=0, keepdim=True))  # dim为0，按照相同的列求和，并在结果中保留列特征\n",
    "print(X.sum(dim=1, keepdim=True))  # dim为1，按照相同的行求和，并在结果中保留行特征\n",
    "print(X.sum(dim=0, keepdim=False)) # dim为0，按照相同的列求和，不在结果中保留列特征\n",
    "print(X.sum(dim=1, keepdim=False)) # dim为1，按照相同的行求和，不在结果中保留行特征\n",
    "\n",
    "'''\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "\n",
    "tensor([[5, 7, 9]])\n",
    "tensor([[ 6],\n",
    "        [15]])\n",
    "tensor([5, 7, 9])\n",
    "tensor([ 6, 15])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.几个重要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view()\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(X)\n",
    "\n",
    "print(X.view(-1, 1))\n",
    "print(X.view(1, -1))\n",
    "print(X.view(-1)) #变为一维\n",
    "\n",
    "'''\n",
    "tensor([[1, 2, 3],\n",
    "        [4, 5, 6]])\n",
    "tensor([[1],\n",
    "        [2],\n",
    "        [3],\n",
    "        [4],\n",
    "        [5],\n",
    "        [6]])\n",
    "tensor([[1, 2, 3, 4, 5, 6]])\n",
    "tensor([1, 2, 3, 4, 5, 6])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather()\n",
    "\n",
    "torch.gather(input, dim, index, out=None) → Tensor(其形状与index一致)\n",
    "\n",
    "#例1：\n",
    "b = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "print(b)\n",
    "index_1 = torch.LongTensor([[0,1],[2,0]])\n",
    "index_2 = torch.LongTensor([[0,1,1],[0,0,0]])\n",
    "print(torch.gather(b, dim=1, index=index_1)) #二维时，1表示行\n",
    "print(torch.gather(b, dim=0, index=index_2)) #二维时，0表示列\n",
    "\n",
    "'''\n",
    "tensor([[1., 2., 3.],\n",
    "        [4., 5., 6.]])\n",
    "tensor([[1., 2.],\n",
    "        [6., 4.]])\n",
    "tensor([[1., 5., 6.],\n",
    "        [1., 2., 3.]])\n",
    "'''\n",
    "\n",
    "#例2：\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = torch.LongTensor([0, 2])\n",
    "y_hat = torch.gather(y_hat, 1, y.view(-1, 1))  #等价于y_hat.gather(1, y.view(-1, 1))\n",
    "print(y_hat)\n",
    "\n",
    "'''\n",
    "tensor([[0.1000],\n",
    "        [0.5000]])\n",
    "'''\n",
    "\n",
    "#例3：\n",
    "a = torch.randint(0, 30, (2, 3, 5))\n",
    "print(a)\n",
    "'''\n",
    "tensor([[[27, 25, 10,  9, 29],\n",
    "         [29, 22, 19, 23, 16],\n",
    "         [19,  9,  0, 25,  3]],\n",
    "\n",
    "        [[11, 28, 26, 25, 24],\n",
    "         [ 3, 14, 11,  3,  6],\n",
    "         [29,  4,  0, 18,  4]]])\n",
    "'''\n",
    "\n",
    "index = torch.LongTensor([[[0,1,2,0,2],\n",
    "                          [0,0,0,0,0],\n",
    "                          [1,1,1,1,1]],\n",
    "                        [[1,2,2,2,2],\n",
    "                         [0,0,0,0,0],\n",
    "                         [2,2,2,2,2]]])\n",
    "print(a.size()==index.size())\n",
    "b = torch.gather(a, 1, index) #三维时，1表示列\n",
    "print(b)\n",
    "'''\n",
    "tensor([[[27, 22,  0,  9,  3],\n",
    "         [27, 25, 10,  9, 29],\n",
    "         [29, 22, 19, 23, 16]],\n",
    "\n",
    "        [[ 3,  4,  0, 18,  4],\n",
    "         [11, 28, 26, 25, 24],\n",
    "         [29,  4,  0, 18,  4]]])\n",
    "'''\n",
    "\n",
    "c = torch.gather(a, 2, index) #三维时，2表示行\n",
    "print(c)\n",
    "'''\n",
    "tensor([[[27, 25, 10, 27, 10],\n",
    "         [29, 29, 29, 29, 29],\n",
    "         [ 9,  9,  9,  9,  9]],\n",
    "\n",
    "        [[28, 26, 26, 26, 26],\n",
    "         [ 3,  3,  3,  3,  3],\n",
    "         [ 0,  0,  0,  0,  0]]])\n",
    "'''\n",
    "\n",
    "index2 = torch.LongTensor([[[0,1,1,0,1],\n",
    "                          [0,1,1,1,1],\n",
    "                          [1,1,1,1,1]],\n",
    "                        [[1,0,0,0,0],\n",
    "                         [0,0,0,0,0],\n",
    "                         [1,1,0,0,0]]])\n",
    "\n",
    "d = torch.gather(a, 0, index2) #三维时，0表示分别从index对应的(n, c, l)的n个不同组元素中选取相应位置元素\n",
    "print(d)\n",
    "'''\n",
    "例如：这里n=2，即一共有2个(3,5)列的数据，[0,1,1,0,1]分别从第一组数据[27,25,10,9,29]\n",
    "中选择第0个位置的数(27),再从第二组数据[11, 28, 26, 25, 24]中选择第1个位置的数(28)，\n",
    "再从第二组数据中选择第3个位置的数(26),再从第一组数据中选择第4个位置的数(9)，最后从\n",
    "第二组数据中选择第5个位置的数(24)\n",
    "a = tensor([[[27, 25, 10,  9, 29],\n",
    "         [29, 22, 19, 23, 16],\n",
    "         [19,  9,  0, 25,  3]],\n",
    "\n",
    "        [[11, 28, 26, 25, 24],\n",
    "         [ 3, 14, 11,  3,  6],\n",
    "         [29,  4,  0, 18,  4]]])\n",
    "         \n",
    "tensor([[[27, 28, 26,  9, 24],\n",
    "         [29, 14, 11,  3,  6],\n",
    "         [29,  4,  0, 18,  4]],\n",
    "\n",
    "        [[11, 25, 10,  9, 29],\n",
    "         [29, 22, 19, 23, 16],\n",
    "         [29,  4,  0, 25,  3]]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU (rectified linear unit)\n",
    "\n",
    "ReLU函数提供了一个简单的非线性变换。给定元素$x$，该函数定义为\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(x, 0).\n",
    "$$\n",
    "\n",
    "可以看出，ReLU函数只保留正数元素，并将负数元素清零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def xyplot(x_vals, y_vals, name):\n",
    "    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel(name + '(x)')\n",
    "    \n",
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = x.relu()\n",
    "xyplot(x, y, 'relu')\n",
    "\n",
    "y.sum().backward()\n",
    "xyplot(x, x.grad, 'grad of relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "\n",
    "sigmoid函数可以将元素的值变换到0和1之间：\n",
    "\n",
    "$$\n",
    "\\text{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.sigmoid()\n",
    "xyplot(x, y, 'sigmoid')\n",
    "\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "xyplot(x, x.grad, 'grad of sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh\n",
    "\n",
    "tanh（双曲正切）函数可以将元素的值变换到-1和1之间，函数关于原点对称：\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.tanh()\n",
    "xyplot(x, y, 'tanh')\n",
    "\n",
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "xyplot(x, x.grad, 'grad of tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于激活函数的选择\n",
    "\n",
    "* ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，`ReLU函数只能在隐藏层中使用`。\n",
    "* 用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。  \n",
    "* 在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。\n",
    "* 在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
