{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram 模型的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42068\n",
      "# tokens: 887100\n",
      "# tokens: 375846\n"
     ]
    }
   ],
   "source": [
    "# 读入数据集\n",
    "\n",
    "with open('D:\\IIE\\pytorch\\jupyter_model\\data\\ptb\\ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines() # 该数据集中句子以换行符为分割\n",
    "    raw_dataset = [st.split() for st in lines] #其中生僻词用unk表示，数字用N表示\n",
    "print('# sentences: %d' % len(raw_dataset))\n",
    "\n",
    "# test\n",
    "#for st in raw_dataset[:3]:\n",
    "#    print('# tokens:', len(st), st[:5]) \n",
    "    \n",
    "    \n",
    "# 建立词语索引\n",
    "counter = collections.Counter([tk for st in raw_dataset for tk in st]) # tk是token的缩写\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items())) # 只保留在数据集中至少出现5次的词\n",
    "\n",
    "idx_to_token = [tk for tk, _ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx, tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]\n",
    "           for st in raw_dataset] # raw_dataset中的单词在这一步被转换为对应的idx\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "print('# tokens: %d' % num_tokens)\n",
    "\n",
    "# 二次采样\n",
    "'''\n",
    "文本数据中一般会出现一些高频词，如英文中的“the”“a”和“in”。通常来说，在一个背景窗口中，\n",
    "一个词（如“chip”）和较低频词（如“microprocessor”）同时出现比和较高频词（如“the”）同时出现对训练词嵌入模型更有益。\n",
    "'''\n",
    "def discard(idx):\n",
    "    '''\n",
    "    @params:\n",
    "        idx: 单词的下标\n",
    "    @return: True/False 表示是否丢弃该单词\n",
    "    '''\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx_to_token[idx]] * num_tokens)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "print('# tokens: %d' % sum([len(st) for st in subsampled_dataset]))\n",
    "\n",
    "# 提取中心词和背景词\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    '''\n",
    "    @params:\n",
    "        dataset: 数据集为句子的集合，每个句子则为单词的集合，此时单词已经被转换为相应数字下标\n",
    "        max_window_size: 背景词的词窗大小的最大值\n",
    "    @return:\n",
    "        centers: 中心词的集合\n",
    "        contexts: 背景词窗的集合，与中心词对应，每个背景词窗则为背景词的集合\n",
    "    '''\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:  # 每个句子至少要有2个词才可能组成一对“中心词-背景词”\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size) # 随机选取背景词窗大小\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(len(st), center_i + 1 + window_size)))\n",
    "            indices.remove(center_i)  # 将中心词排除在背景词之外\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "\n",
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "\n",
    "# 定义跳字模型\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    '''\n",
    "    @params:\n",
    "        center: 中心词下标，形状为 (n, 1) 的整数张量\n",
    "        contexts_and_negatives: 背景词和噪音词下标，形状为 (n, m) 的整数张量\n",
    "        embed_v: 中心词的 embedding 层\n",
    "        embed_u: 背景词的 embedding 层\n",
    "    @return:\n",
    "        pred: 中心词与背景词（或噪音词）的内积，之后可用于计算概率 p(w_o|w_c)\n",
    "    '''\n",
    "    v = embed_v(center) # shape of (n, 1, d)\n",
    "    u = embed_u(contexts_and_negatives) # shape of (n, m, d)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) # bmm((n, 1, d), (n, d, m)) => shape of (n, 1, m)\n",
    "    return pred\n",
    "\n",
    "# 负采样\n",
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    '''\n",
    "    @params:\n",
    "        all_contexts: [[w_o1, w_o2, ...], [...], ... ]\n",
    "        sampling_weights: 每个单词的噪声词采样概率\n",
    "        K: 随机采样个数\n",
    "    @return:\n",
    "        all_negatives: [[w_n1, w_n2, ...], [...], ...]\n",
    "    '''\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                # 根据每个词的权重（sampling_weights）随机生成k个词的索引作为噪声词。\n",
    "                # 为了高效计算，可以将k设得稍大一点\n",
    "                i, neg_candidates = 0, random.choices(\n",
    "                    population, sampling_weights, k=int(1e5))\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "            # 噪声词不能是背景词\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "sampling_weights = [counter[w]**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "# 批量读取数据\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "def batchify(data):\n",
    "    '''\n",
    "    用作DataLoader的参数collate_fn\n",
    "    @params:\n",
    "        data: 长为batch_size的列表，列表中的每个元素都是__getitem__得到的结果\n",
    "    @outputs:\n",
    "        batch: 批量化后得到 (centers, contexts_negatives, masks, labels) 元组\n",
    "            centers: 中心词下标，形状为 (n, 1) 的整数张量\n",
    "            contexts_negatives: 背景词和噪声词的下标，形状为 (n, m) 的整数张量\n",
    "            masks: 与补齐相对应的掩码，形状为 (n, m) 的0/1整数张量\n",
    "            labels: 指示中心词的标签，形状为 (n, m) 的0/1整数张量\n",
    "    '''\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)] # 使用掩码变量mask来避免填充项对损失函数计算的影响\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "        batch = (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "    return batch\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify, \n",
    "                            num_workers=num_workers)\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks',\n",
    "                           'labels'], batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8740, 1.2100])\n"
     ]
    }
   ],
   "source": [
    "# 损失函数\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        @params:\n",
    "            inputs: 经过sigmoid层后为预测D=1的概率\n",
    "            targets: 0/1向量，1代表背景词，0代表噪音词\n",
    "        @return:\n",
    "            res: 平均到每个label的loss\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        res = res.sum(dim=1) / mask.float().sum(dim=1)\n",
    "        return res\n",
    "\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]]) # 标签变量label中的1和0分别代表背景词和噪声词\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])  # 掩码变量\n",
    "print(loss(pred, label, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练与测试\n",
    "\n",
    "embed_size = 100\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "                    nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size))\n",
    "\n",
    "def train(net, lr, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            \n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            \n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs'\n",
    "              % (epoch + 1, l_sum / n, time.time() - start))\n",
    "\n",
    "train(net, 0.01, 5)\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    '''\n",
    "    @params:\n",
    "        query_token: 给定的词语\n",
    "        k: 近义词的个数\n",
    "        embed: 预训练词向量\n",
    "    '''\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    # 添加的1e-9是为了数值稳定性\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1) * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:  # 除去输入词\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "        \n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
